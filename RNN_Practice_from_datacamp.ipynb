{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Practice from datacamp",
      "provenance": [],
      "authorship_tag": "ABX9TyNaubnaigyNKsYJ/IuyrgYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushiai/data_csv/blob/main/RNN_Practice_from_datacamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7h8rWxooxTa"
      },
      "source": [
        "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
        " 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.',\n",
        " 'For example, I cry because others are stupid, and that makes me sad.',\n",
        " \"I'm not insane, my mother had me tested.\",\n",
        " 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.',\n",
        " \"Amy's birthday present will be my genitals.\",\n",
        " '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!',\n",
        " 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.',\n",
        " 'I would have been here sooner but the bus kept stopping for other people to get on it.',\n",
        " 'Oh gravity, thou art a heartless bitch.',\n",
        " 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.',\n",
        " 'Well, today we tried masturbating for money.',\n",
        " 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.',\n",
        " \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
        " \"What computer do you have? And please don't say a white one.\",\n",
        " \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
        " 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvU-CoG1VV2a"
      },
      "source": [
        "all_words = ' '.join(sheldon_quotes).split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op6dydsJVcmc",
        "outputId": "15e7e9b3-a659-43e9-d963-96a8368a9ead"
      },
      "source": [
        "all_words[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"You're\", 'afraid', 'of', 'insects', 'and']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHtxlNrNVden"
      },
      "source": [
        "unique_words = list(set(all_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRVPWqGsVllH",
        "outputId": "5ae57085-0b63-4ba1-e48b-1e4f82698707"
      },
      "source": [
        "len(unique_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7pPo_LTV2WD",
        "outputId": "2bc6d19c-c471-4a27-ed09-c1728f29f402"
      },
      "source": [
        "len(all_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSqsrMlaV6PF"
      },
      "source": [
        "index_to_word = {i:wd for i, wd in enumerate(unique_words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBTRS4jeWFmT"
      },
      "source": [
        "word_to_index = {wd:i for i,wd in enumerate(unique_words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXpESmUHWoA7"
      },
      "source": [
        "# Create lists to keep the sentences and the next character\n",
        "sentences = []\n",
        "nex_char = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFfryMC1WqLl"
      },
      "source": [
        "step = 2\n",
        "char_window = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLui6JuEWqJk"
      },
      "source": [
        "for i in range(0,len(sheldon_quotes)-char_window,step):\n",
        "    sentences.append(sheldon_quotes[i:i+char_window])\n",
        "    nex_char.append(sheldon_quotes[i+char_window])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM8FT945WqFe"
      },
      "source": [
        "new_text = ['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away',\n",
        " 'To the brave crew and passengers of the Kobayshi Maru sucks to be you',\n",
        " 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies',\n",
        " 'They are merely scars not mortal wounds and you must use them to propel you forward',\n",
        " 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6tUck7QbsMv"
      },
      "source": [
        "new_text_split = []\n",
        "for sentence in new_text:\n",
        "    split_text = []\n",
        "    for wd in sentence.split():\n",
        "        index = word_to_index.get(wd, 0)\n",
        "        split_text.append(index)\n",
        "    new_text_split.append(split_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtD-PM7Mbufw",
        "outputId": "585a0a1f-188d-427f-f95f-9820213b9cad"
      },
      "source": [
        "new_text_split[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 45, 0, 0, 120, 0, 192, 45, 0, 0, 0, 142, 200, 72]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DiaQ-O72f0LG",
        "outputId": "c4092164-35d8-4da6-f965-c9e7caf53c4a"
      },
      "source": [
        "new_text[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'To the brave crew and passengers of the Kobayshi Maru sucks to be you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ekcyla6QgcM0",
        "outputId": "b25bca82-5b79-4ba9-cfcb-171d289ec9e7"
      },
      "source": [
        "index_to_word.get(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'became'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V7M_Jdud6Sf",
        "outputId": "9276f6ca-d5fb-415e-a9b6-78bb6f8f0678"
      },
      "source": [
        "print(' '.join([index_to_word[indexx] for indexx in new_text_split[1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "became the became became and became of the became became became to be you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VeWcze2mPdC"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etmk4U3lfR5d",
        "outputId": "6fb3e3ac-063c-4d5d-86ae-4b6dc36cf01f"
      },
      "source": [
        "# Instantiate the class\n",
        "model = Sequential(name='sequential_model')\n",
        "\n",
        "# One LSTM layer (defining the input shape because it is the \n",
        "# initial layer)\n",
        "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
        "\n",
        "# Add a dense layer with one unit\n",
        "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
        "\n",
        "# The summary shows the layers and the number of parameters \n",
        "# that will be trained\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "LSTM (LSTM)                  (None, 128)               71168     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 71,297\n",
            "Trainable params: 71,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMWyS510d6NL"
      },
      "source": [
        "# Model = Sequential(name='sequential_model')\n",
        "# Define the input layer\n",
        "main_input = Input(shape=(None, 10), name=\"input\")\n",
        "\n",
        "# One LSTM layer (input shape is already defined)\n",
        "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
        "\n",
        "# Add a dense layer with one unit\n",
        "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
        "\n",
        "# Instantiate the class at the end\n",
        "model = model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
        "\n",
        "# Same amount of parameters to train as before (71,297)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9-AUxCwSeaC"
      },
      "source": [
        "Keras preprocessing\n",
        "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
        "\n",
        "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
        "\n",
        "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-CK4AR5R_2I"
      },
      "source": [
        "texts = ['Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie.',\n",
        "         'Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBa9_DdknH6a"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLOSvnt-Rxs8"
      },
      "source": [
        "# Build the dictionary of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAI6Fun6Uf3h",
        "outputId": "cea48c0e-5f5e-4dbe-f17d-36c489ffca09"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'about': 18,\n",
              " 'allow': 14,\n",
              " 'and': 10,\n",
              " 'any': 57,\n",
              " 'are': 58,\n",
              " 'became': 38,\n",
              " 'blood': 40,\n",
              " 'born': 22,\n",
              " 'by': 43,\n",
              " 'can': 60,\n",
              " 'can’t': 55,\n",
              " 'children': 13,\n",
              " 'co': 27,\n",
              " 'curie': 26,\n",
              " 'death': 42,\n",
              " 'discoverer': 28,\n",
              " 'discovery': 45,\n",
              " 'done': 59,\n",
              " 'educated': 24,\n",
              " 'fell': 34,\n",
              " 'female': 6,\n",
              " 'filled': 39,\n",
              " 'french': 23,\n",
              " 'go': 61,\n",
              " 'great': 19,\n",
              " 'hair': 33,\n",
              " 'happen': 56,\n",
              " 'hard': 47,\n",
              " 'hello': 12,\n",
              " 'her': 5,\n",
              " 'hero': 30,\n",
              " 'i': 49,\n",
              " 'inspire': 16,\n",
              " 'little': 46,\n",
              " 'madame': 25,\n",
              " 'me': 15,\n",
              " 'no': 51,\n",
              " 'of': 4,\n",
              " 'out': 35,\n",
              " 'own': 44,\n",
              " 'poisoned': 41,\n",
              " 'polish': 21,\n",
              " 'radioactivity': 29,\n",
              " 'reason': 52,\n",
              " 'science': 31,\n",
              " 'scientist': 20,\n",
              " 'see': 50,\n",
              " 'she': 8,\n",
              " 'stool': 37,\n",
              " 'story': 17,\n",
              " 'that': 54,\n",
              " 'to': 2,\n",
              " 'until': 32,\n",
              " 'vomit': 36,\n",
              " 'was': 9,\n",
              " 'we': 11,\n",
              " 'why': 53,\n",
              " 'with': 3,\n",
              " 'work': 48,\n",
              " 'you': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFUnHq8XSSnT"
      },
      "source": [
        "# Change texts into sequence of indexes\n",
        "texts_numeric = tokenizer.texts_to_sequences(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9CBUiVTS03H",
        "outputId": "960ccfab-041a-408b-be3e-c41fa2a03b21"
      },
      "source": [
        "print(texts_numeric[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12, 6, 13, 14, 15, 2, 16, 7, 3, 1, 17, 18, 1, 19, 6, 20, 21, 22, 23, 24, 25, 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5muuoP3S2Gu",
        "outputId": "7e065216-3a1a-45ee-b05f-bb3db6323f1d"
      },
      "source": [
        "print('No of word in sample texts ({0},{1})'.format(len(texts_numeric[0]), len(texts_numeric[1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of word in sample texts (22,56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089msD6NUFWC"
      },
      "source": [
        "texts_pad = pad_sequences(texts_numeric,60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9E8HlTnVADL",
        "outputId": "be5fb929-c9e5-4bc0-8818-3b2cc72d93cb"
      },
      "source": [
        "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now the texts have fixed length: 60. Let's see the first one: \n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  6 13 14 15  2 16  7  3  1\n",
            " 17 18  1 19  6 20 21 22 23 24 25 26]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wva_zlzru5a5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEsBoYSMu6Rr"
      },
      "source": [
        "# Exploding gradient problem\n",
        "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
        "\n",
        "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
        "\n",
        "The data is already loaded on the environment as X_train, X_test, y_train and y_test.\n",
        "\n",
        "You will use a Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) as the loss function.\n",
        "\n",
        "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the clipvalue parameter to solve the problem.\n",
        "\n",
        "The Stochastic Gradient Descent in Keras is loaded as SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIg_nSKzucR"
      },
      "source": [
        "# Stacking RNN layers\n",
        "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
        "\n",
        "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
        "\n",
        "To do this, you will set the return_sequences argument to True on the firsts two LSTM layers and to False on the last LSTM layer.\n",
        "\n",
        "To create models with even more layers, you can keep adding them one after the other or create a function that uses the .add() method inside a loop to add many layers with few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrELd4Dwu5Yo"
      },
      "source": [
        "from keras.layers import LSTM, Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhiezRAnu5Uc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(None,1), return_sequences=True))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(LSTM(128,return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQOhGYlQu5P9",
        "outputId": "6aaa88db-79e3-4b53-90c9-7ee53e42ea0e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_3 (LSTM)                (None, None, 128)         66560     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 128)         131584    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 329,857\n",
            "Trainable params: 329,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-50ozgm5qu8"
      },
      "source": [
        "#Transfer learning\n",
        "You saw that when training an embedding layer, you need to learn a lot of parameters.\n",
        "\n",
        "In this exercise, you will see that when using transfer learning it is possible to use the pre-trained weights and don't update them, meaning that all the parameters of the embedding layer will be fixed, and the model will only need to learn the parameters from the other layers.\n",
        "\n",
        "The function load_glove is already loaded on the environment and retrieves the glove matrix as a numpy.ndarray vector. It uses the function covered on the lesson's slides to retrieve the glove vectors with 200 embedding dimensions for the vocabulary present in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVqQ24zMu5Nb"
      },
      "source": [
        "# Load the glove pre-trained vectors\n",
        "glove_matrix = load_glove('glove_200d.zip')\n",
        "\n",
        "# Create a model with embeddings\n",
        "model = Sequential(name=\"emb_model\")\n",
        "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
        "                    embeddings_initializer=Constant(glove_matrix), \n",
        "                    input_length=sentence_len, trainable=False))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the summaries of the model with embeddings\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4qjydBu5JN"
      },
      "source": [
        "# Create the model with embedding\n",
        "model = Sequential(name=\"emb_model\")\n",
        "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
        "model.add(SimpleRNN(units=128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('embedding_model_weights.h5')\n",
        "\n",
        "# Evaluate the models' performance (ignore the loss value)\n",
        "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the results\n",
        "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))\n",
        "\n",
        "# <script.py> output:\n",
        "#     SimpleRNN model's accuracy:\t0.495\n",
        "#     Embeddings model's accuracy:\t0.733"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9kCUacI7QvT"
      },
      "source": [
        "#Better sentiment classification\n",
        "In this exercise, you go back to the sentiment classification problem seen in Chapter 1.\n",
        "\n",
        "You are going to add more complexity to the model and improve its accuracy. You will use an Embedding layer to train word vectors on the training set and two LSTM layers to keep track of longer texts. Also, you will add an extra Dense layer before the output.\n",
        "\n",
        "This is no longer a simple model, and the training can take some time. For this reason, a pre-trained model is available by loading its weights with the method .load_weights() from the keras.models.Sequential class. The model was trained with 10 epochs and its weights are available on the file model_weights.h5.\n",
        "\n",
        "The following modules are loaded on the environment: Sequential, Embedding, LSTM, Dropout, Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TplM04-Tu5HW"
      },
      "source": [
        "# Build and compile the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
        "model.add(Dense(16))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights('model_weights.h5')\n",
        "\n",
        "# Print the obtained loss and accuracy\n",
        "print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0eOoqil8sDG"
      },
      "source": [
        "#Using the CNN layer\n",
        "In this exercise, you will use a pre-trained model that makes use of the Conv1D and MaxPooling1D layers from the keras.layers.convolutional module, and achieves even better accuracy on the classification task.\n",
        "\n",
        "This architecture achieved good results in language modeling tasks such as classification, and is added here as an extra exercise to see it in action and have some intuitions.\n",
        "\n",
        "Because this layer is not in the scope of the course, you will focus on how to use the layers together with the RNN layers you already learned.\n",
        "\n",
        "Please follow the instructions to see the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gflqp6Uzu5FM"
      },
      "source": [
        "# Print the model summary\n",
        "model_cnn.summary()\n",
        "\n",
        "# Load pre-trained weights\n",
        "model_cnn.load_weights('model_weights.h5')\n",
        "\n",
        "# Evaluate the model to get the loss and accuracy values\n",
        "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print the loss and accuracy obtained\n",
        "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))\n",
        "\n",
        "# Loss: 0.4343099966049194\n",
        "# Accuracy: 0.836"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVrC6Hlbu5C4"
      },
      "source": [
        "<script.py> output:\n",
        "    Model: \"cnn_model\"\n",
        "    _________________________________________________________________\n",
        "    Layer (type)                 Output Shape              Param #   \n",
        "    =================================================================\n",
        "    Embedding (Embedding)        (None, 800, 100)          2000100   \n",
        "    _________________________________________________________________\n",
        "    dropout_1 (Dropout)          (None, 800, 100)          0         \n",
        "    _________________________________________________________________\n",
        "    Conv (Conv1D)                (None, 797, 16)           6416      \n",
        "    _________________________________________________________________\n",
        "    MaxPool (MaxPooling1D)       (None, 398, 16)           0         \n",
        "    _________________________________________________________________\n",
        "    dropout_2 (Dropout)          (None, 398, 16)           0         \n",
        "    _________________________________________________________________\n",
        "    LSTM (LSTM)                  (None, 64)                20736     \n",
        "    _________________________________________________________________\n",
        "    dropout_3 (Dropout)          (None, 64)                0         \n",
        "    _________________________________________________________________\n",
        "    Dense2 (Dense)               (None, 16)                1040      \n",
        "    _________________________________________________________________\n",
        "    Output (Dense)               (None, 1)                 17        \n",
        "    =================================================================\n",
        "    Total params: 2,028,309\n",
        "    Trainable params: 2,028,309\n",
        "    Non-trainable params: 0\n",
        "    _________________________________________________________________\n",
        "    Loss: 0.4343099966049194\n",
        "    Accuracy: 0.836"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPZW2Wetu5Ao"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JffGP25_u4-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0BgClBrh6yO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cf3tMr2cscl"
      },
      "source": [
        "# Prepare label vectors\n",
        "In the video exercise, you learned the differences between binary classification and multi-class classification. You learned that there are some modifications to the data preparation process that need to be done before training the models.\n",
        "\n",
        "In this exercise, you will prepare a raw dataset with labels given as text. The data is given as a pandas.DataFrame called df, with two columns: text with the text data and label with the label names. Your task is to make all the necessary transformations to the labels: change string to number and one-hot encode.\n",
        "\n",
        "The module pandas as pd and the function to_categorical() from keras.utils.np_utils are already loaded in the environment and the first lines of the dataset is printed on the console for you to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXHBu3c8dBJb"
      },
      "source": [
        "# Get the numerical ids of column label\n",
        "numerical_ids = df.label.cat.codes\n",
        "\n",
        "# Print initial shape\n",
        "print(numerical_ids.shape)\n",
        "\n",
        "# One-hot encode the indexes\n",
        "Y = to_categorical(numerical_ids)\n",
        "\n",
        "# Check the new shape of the variable\n",
        "print(Y.shape)\n",
        "\n",
        "# Print the first 5 rows\n",
        "print(Y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjMLMoJJeWi0"
      },
      "source": [
        "# Pre-process data\n",
        "You learned the differences for pre-processing the data in the case of multi-class classification. Let's put that into practice by preprocessing the data in anticipation of creating a simple multi-class classification model.\n",
        "\n",
        "The dataset is loaded in the variable news_dataset, and has the following attributes:\n",
        "\n",
        "news_dataset.data: array with texts\n",
        "news_dataset.target: array with target categories as numerical indexes\n",
        "The sample data contains 5,000 observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7njUbd41dF58"
      },
      "source": [
        "# Create and fit tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(news_dataset.data)\n",
        "\n",
        "# Prepare the data\n",
        "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
        "prep_data = pad_sequences(prep_data, maxlen=200)\n",
        "\n",
        "# Prepare the labels\n",
        "prep_labels = to_categorical(news_dataset.target)\n",
        "\n",
        "# Print the shapes\n",
        "print(prep_data.shape)\n",
        "print(prep_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-At8U3Augcnz"
      },
      "source": [
        "#Transfer learning starting point\n",
        "In this exercise you will see the benefit of using pre-trained vectors as a starting point for your model.\n",
        "\n",
        "You will compare the accuracy of two models trained with two epochs. The architecture of the models is the same: One embedding layer, one LSTM layer with 128 units and the output layer with 5 units which is the number of classes in the sample data. The difference is that one model uses pre-trained vectors on the embedding layer (transfer learning) and the other doesn't.\n",
        "\n",
        "The pre-trained vectors used were the GloVE with 200 dimension. The training accuracy history of the validation set of both models are available in the variables history_no_emb and history_emb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVPOXseygdiP"
      },
      "source": [
        "# Import plotting package\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Insert lists of accuracy obtained on the validation set\n",
        "plt.plot(history_no_emb['acc'], marker='o')\n",
        "plt.plot(history_emb['acc'], marker='o')\n",
        "\n",
        "# Add extra descriptions to plot\n",
        "plt.title('Learning with and without pre-trained embedding vectors')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvAOKxZcggsX"
      },
      "source": [
        "# Word2Vec\n",
        "In this exercise you will create a Word2Vec model using Keras.\n",
        "\n",
        "The corpus used to pre-train the model is the script of all episodes of the The Big Bang Theory TV show, divided sentence by sentence. It is available in the variable bigbang.\n",
        "\n",
        "The text on the corpus was transformed to lower case and all words were tokenized. The result is stored in the tokenized_corpus variable.\n",
        "\n",
        "A Word2Vec model was pre-trained using a window size of 10 words for context (5 before and 5 after the center word), words with less than 3 occurrences were removed and the skip gram model method was used with 50 dimension. The model is saved on the file bigbang_word2vec.model.\n",
        "\n",
        "The class Word2Vec is already loaded in the environment from gensim.models.word2vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V0dr1uOi-Bn"
      },
      "source": [
        "# Word2Vec model\n",
        "w2v_model = Word2Vec.load(\"bigbang_word2vec.model\")\n",
        "\n",
        "# Selected words to check similarities\n",
        "words_of_interest = ['bazinga', 'penny', 'universe', 'spock', 'brain']\n",
        "\n",
        "# Compute top 5 similar words for each of the words of interest\n",
        "top5_similar_words = []\n",
        "for word in words_of_interest:\n",
        "    top5_similar_words.append(\n",
        "      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n",
        "    )\n",
        "\n",
        "# Print the similar words\n",
        "print(top5_similar_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz2Ct1L7kNBM"
      },
      "source": [
        "#Exploring 20 News Groups dataset\n",
        "In this exercise, you will be given a sample of the 20 News Groups dataset obtained using the fetch_20newsgroups() function from sklearn.datasets, filtering only three classes: sci.space, alt.atheism and soc.religion.christian.\n",
        "\n",
        "The dataset is loaded in the variable news_dataset. Its attributes are printed so you can explore them on the console.\n",
        "\n",
        "Fore more details on how to use this function, see the Sklearn documentation.\n",
        "\n",
        "You will tokenize the texts and one-hot encode the labels step by step to understand how the transformations happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROb4onBjkOCn"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zknlf2F0kTzv",
        "outputId": "bc022842-266c-429c-f21a-b6f52d214c61"
      },
      "source": [
        "data = fetch_20newsgroups()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpmmqepbkYie",
        "outputId": "b56e3911-4200-49f0-aadf-dda6eca9f68c"
      },
      "source": [
        "print(data.DESCR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _20newsgroups_dataset:\n",
            "\n",
            "The 20 newsgroups text dataset\n",
            "------------------------------\n",
            "\n",
            "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
            "20 topics split in two subsets: one for training (or development)\n",
            "and the other one for testing (or for performance evaluation). The split\n",
            "between the train and test set is based upon a messages posted before\n",
            "and after a specific date.\n",
            "\n",
            "This module contains two loaders. The first one,\n",
            ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
            "returns a list of the raw texts that can be fed to text feature\n",
            "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
            "with custom parameters so as to extract feature vectors.\n",
            "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
            "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
            "extractor.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ==========\n",
            "    Classes                     20\n",
            "    Samples total            18846\n",
            "    Dimensionality               1\n",
            "    Features                  text\n",
            "    =================   ==========\n",
            "\n",
            "Usage\n",
            "~~~~~\n",
            "\n",
            "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
            "fetching / caching functions that downloads the data archive from\n",
            "the original `20 newsgroups website`_, extracts the archive contents\n",
            "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
            ":func:`sklearn.datasets.load_files` on either the training or\n",
            "testing set folder, or both of them::\n",
            "\n",
            "  >>> from sklearn.datasets import fetch_20newsgroups\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
            "\n",
            "  >>> from pprint import pprint\n",
            "  >>> pprint(list(newsgroups_train.target_names))\n",
            "  ['alt.atheism',\n",
            "   'comp.graphics',\n",
            "   'comp.os.ms-windows.misc',\n",
            "   'comp.sys.ibm.pc.hardware',\n",
            "   'comp.sys.mac.hardware',\n",
            "   'comp.windows.x',\n",
            "   'misc.forsale',\n",
            "   'rec.autos',\n",
            "   'rec.motorcycles',\n",
            "   'rec.sport.baseball',\n",
            "   'rec.sport.hockey',\n",
            "   'sci.crypt',\n",
            "   'sci.electronics',\n",
            "   'sci.med',\n",
            "   'sci.space',\n",
            "   'soc.religion.christian',\n",
            "   'talk.politics.guns',\n",
            "   'talk.politics.mideast',\n",
            "   'talk.politics.misc',\n",
            "   'talk.religion.misc']\n",
            "\n",
            "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
            "attribute is the integer index of the category::\n",
            "\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (11314,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
            "\n",
            "It is possible to load only a sub-selection of the categories by passing the\n",
            "list of the categories to load to the\n",
            ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
            "\n",
            "  >>> cats = ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
            "\n",
            "  >>> list(newsgroups_train.target_names)\n",
            "  ['alt.atheism', 'sci.space']\n",
            "  >>> newsgroups_train.filenames.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target.shape\n",
            "  (1073,)\n",
            "  >>> newsgroups_train.target[:10]\n",
            "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "\n",
            "Converting text to vectors\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "In order to feed predictive or clustering models with the text data,\n",
            "one first need to turn the text into vectors of numerical values suitable\n",
            "for statistical analysis. This can be achieved with the utilities of the\n",
            "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
            "example that extract `TF-IDF`_ vectors of unigram tokens\n",
            "from a subset of 20news::\n",
            "\n",
            "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
            "  ...               'comp.graphics', 'sci.space']\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectorizer = TfidfVectorizer()\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> vectors.shape\n",
            "  (2034, 34118)\n",
            "\n",
            "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
            "components by sample in a more than 30000-dimensional space\n",
            "(less than .5% non-zero features)::\n",
            "\n",
            "  >>> vectors.nnz / float(vectors.shape[0])\n",
            "  159.01327...\n",
            "\n",
            ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
            "returns ready-to-use token counts features instead of file names.\n",
            "\n",
            ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
            ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
            "\n",
            "\n",
            "Filtering text for more realistic training\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "It is easy for a classifier to overfit on particular things that appear in the\n",
            "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
            "high F-scores, but their results would not generalize to other documents that\n",
            "aren't from this window of time.\n",
            "\n",
            "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
            "which is fast to train and achieves a decent F-score::\n",
            "\n",
            "  >>> from sklearn.naive_bayes import MultinomialNB\n",
            "  >>> from sklearn import metrics\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.88213...\n",
            "\n",
            "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
            "the training and test data, instead of segmenting by time, and in that case\n",
            "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
            "yet of what's going on inside this classifier?)\n",
            "\n",
            "Let's take a look at what the most informative features are:\n",
            "\n",
            "  >>> import numpy as np\n",
            "  >>> def show_top10(classifier, vectorizer, categories):\n",
            "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
            "  ...     for i, category in enumerate(categories):\n",
            "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
            "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
            "  ...\n",
            "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
            "  alt.atheism: edu it and in you that is of to the\n",
            "  comp.graphics: edu in graphics it is for and of to the\n",
            "  sci.space: edu it that is in and space to of the\n",
            "  talk.religion.misc: not it you in is that and to of the\n",
            "\n",
            "\n",
            "You can now see many things that these features have overfit to:\n",
            "\n",
            "- Almost every group is distinguished by whether headers such as\n",
            "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
            "- Another significant feature involves whether the sender is affiliated with\n",
            "  a university, as indicated either by their headers or their signature.\n",
            "- The word \"article\" is a significant feature, based on how often people quote\n",
            "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
            "  wrote:\"\n",
            "- Other features match the names and e-mail addresses of particular people who\n",
            "  were posting at the time.\n",
            "\n",
            "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
            "barely have to identify topics from text at all, and they all perform at the\n",
            "same high level.\n",
            "\n",
            "For this reason, the functions that load 20 Newsgroups data provide a\n",
            "parameter called **remove**, telling it what kinds of information to strip out\n",
            "of each file. **remove** should be a tuple containing any subset of\n",
            "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
            "blocks, and quotation blocks respectively.\n",
            "\n",
            "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
            "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                      categories=categories)\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
            "  0.77310...\n",
            "\n",
            "This classifier lost over a lot of its F-score, just because we removed\n",
            "metadata that has little to do with topic classification.\n",
            "It loses even more if we also strip this metadata from the training data:\n",
            "\n",
            "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
            "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
            "  ...                                       categories=categories)\n",
            "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
            "  >>> clf = MultinomialNB(alpha=.01)\n",
            "  >>> clf.fit(vectors, newsgroups_train.target)\n",
            "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "\n",
            "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
            "  >>> pred = clf.predict(vectors_test)\n",
            "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
            "  0.76995...\n",
            "\n",
            "Some other classifiers cope better with this harder version of the task. Try\n",
            "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
            "the ``--filter`` option to compare the results.\n",
            "\n",
            ".. topic:: Recommendation\n",
            "\n",
            "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
            "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
            "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
            "  lower because it is more realistic.\n",
            "\n",
            ".. topic:: Examples\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
            "\n",
            "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "8WVg-sDckY3B",
        "outputId": "0bdc43f6-6ccc-4cc3-d239-a1c0f9672f46"
      },
      "source": [
        "# sci.space, alt.atheism and soc.religion.christian"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dd4b85dad78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matheism\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreligion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchristian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sci' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ2XCCQPk5I9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJCXGn-lhQ3"
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpV9kfuZlh6E",
        "outputId": "f781f081-2605-452e-ef01-b5f8319ac70b"
      },
      "source": [
        "newsgroups_train.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 4, 4, ..., 3, 1, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBRNBcdHk8Kw"
      },
      "source": [
        "df = pd.DataFrame(fetch_20newsgroups(subset='train')['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS0LVXTMk_oO"
      },
      "source": [
        "df['target'] = fetch_20newsgroups(subset='train')['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xshYvaQQmk4c"
      },
      "source": [
        "# newsgroups_train.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itEvmWhznQPq"
      },
      "source": [
        "arr = [0,1,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjD-Vz34k_z7"
      },
      "source": [
        "df = df[df['target'].isin(arr)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrN6YyxPo1uF"
      },
      "source": [
        "df.rename(columns = {0:'data'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dERqVkFavhn_"
      },
      "source": [
        "df.reset_index(inplace=True,drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA67OcxxvsvQ"
      },
      "source": [
        "X = df['data']\n",
        "y = df['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krgQnCv_vdje"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxuU-jQaoHJW"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LebrKdcmgwe"
      },
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVQdeHR4nlYl",
        "outputId": "96649ee6-2b17-4b63-8447-0ccd2d8e6cb7"
      },
      "source": [
        "# Transform the text into numerical indexes\n",
        "tokenizer.fit_on_texts(X_train) \n",
        "news_num_indices = tokenizer.texts_to_sequences(X_train)\n",
        "print(news_num_indices[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[25, 19725, 7739, 23, 13710, 19726, 53, 162, 728, 17, 534, 7, 3392, 1259, 135, 58, 188, 3820, 367, 97, 54, 28, 109, 99, 104, 10888, 7739, 23, 322, 291, 1997, 789, 139, 813, 3, 13711, 3393, 166, 11, 181, 7, 162, 60, 728, 17, 534, 1259, 135, 102, 291, 1997, 37, 789, 105, 154, 29, 159, 60, 728, 229, 621, 4354, 75, 115, 5, 3, 305, 17, 534, 1997, 252, 46, 13, 274, 225, 6, 71, 308, 60, 728, 17, 83, 252, 12, 57, 30, 667, 25, 70, 3393, 172, 13710]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AADXEy1Ton7k",
        "outputId": "c9be9226-6958-4267-b786-ff8ed8b51d08"
      },
      "source": [
        "labels_onehot = to_categorical(y_train)\n",
        "labels_onehot[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ogR_OshrQHV"
      },
      "source": [
        "#Classifying news articles\n",
        "In this exercise you will create a multi-class classification model.\n",
        "\n",
        "The dataset is already loaded in the environment as news_novel. Also, all the pre-processing of the training data is already done and tokenizer is also available in the environment.\n",
        "\n",
        "A RNN model was pre-trained with the following architecture: use the Embedding layer, one LSTM layer and the output Dense layer expecting three classes: sci.space, alt.atheism, and soc.religion.christian. The weights of this trained model are available on the classify_news_weights.h5 file.\n",
        "\n",
        "You will pre-process the novel data and evaluate on a new dataset news_novel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51MKOf_sssEC"
      },
      "source": [
        "```\n",
        "# Change text for numerical ids and pad\n",
        "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
        "X_novel = pad_sequences(X_novel, maxlen=400)\n",
        "\n",
        "# One-hot encode the labels\n",
        "Y_novel = to_categorical(news_novel.target)\n",
        "\n",
        "# Load the model pre-trained weights\n",
        "model.load_weights('classify_news_weights.h5')\n",
        "\n",
        "# Evaluate the model on the new dataset\n",
        "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
        "\n",
        "# Print the loss and accuracy obtained\n",
        "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8MPVziaqudO"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7EcD8T4sRNO",
        "outputId": "3178235d-efc7-4dfc-9922-27fa977a13ee"
      },
      "source": [
        "news_num_indices = pad_sequences(news_num_indices,maxlen=400)\n",
        "news_num_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13706,    86,   619, ...,    34,     6,  6806],\n",
              "       [    0,     0,     0, ...,  3393,   172, 13710],\n",
              "       [    0,     0,     0, ...,    41,    33,  1017],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,  3410,  6830,  6831],\n",
              "       [    0,     0,     0, ...,    31,   802,   976],\n",
              "       [47497,  3130,     5, ...,  2770,     4,   494]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb1tbrdVtCjk"
      },
      "source": [
        "# Model build (personal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQV4mR6WwPQz"
      },
      "source": [
        "```\n",
        " Build the model\n",
        " model = Sequential()\n",
        " model.add(Embedding(10000, 128))\n",
        " model.add(LSTM(128, dropout=0.2))\n",
        " # Output layer has `num_classes` units and uses `softmax`\n",
        " model.add(Dense(num_classes, activation=\"softmax\"))\n",
        " # Compile the model\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXOykF7_sfDk"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4GSSZrstNQG"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=50000,output_dim=128,input_length=400))\n",
        "model.add(LSTM(128,dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXOpIq7ixJ-t",
        "outputId": "c5eaddcb-bfb2-45a8-ab2c-47846adeb845"
      },
      "source": [
        "news_num_indices.shape, labels_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1324, 400), (1324, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNYx_KZey9aR"
      },
      "source": [
        "# news_num_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIVSx_iSt9Pk",
        "outputId": "e05f02f2-d8f7-48bf-a7e6-516bfcd2de0a"
      },
      "source": [
        "model.fit(news_num_indices,labels_onehot,batch_size= 64,epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "21/21 [==============================] - 16s 680ms/step - loss: 1.0923 - accuracy: 0.3815\n",
            "Epoch 2/3\n",
            "21/21 [==============================] - 14s 674ms/step - loss: 0.9404 - accuracy: 0.7578\n",
            "Epoch 3/3\n",
            "21/21 [==============================] - 14s 670ms/step - loss: 0.5525 - accuracy: 0.7859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd6c5ba72d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HrIA-r82cPc"
      },
      "source": [
        "# tokenizer.fit_on_texts(X_train) \n",
        "test_news_num_indices = tokenizer.texts_to_sequences(X_test)\n",
        "test_news_num_indices = pad_sequences(test_news_num_indices,maxlen=400)\n",
        "test_labels_onehot = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBh-seLb3Rel",
        "outputId": "f84422d1-bdd0-4dec-de55-aa1d712fa75d"
      },
      "source": [
        "test_news_num_indices.shape, test_labels_onehot.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((331, 400), (331, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdnsLUHC3Dnu"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahfXLjqi3bA1",
        "outputId": "7881835d-4338-470a-eb0d-247beb32baea"
      },
      "source": [
        "model.evaluate(test_news_num_indices,test_labels_onehot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 1s 85ms/step - loss: 0.5486 - accuracy: 0.7644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5485668778419495, 0.7643504738807678]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIY9jSIruFG2"
      },
      "source": [
        "y_pred = model.predict(test_news_num_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSnT1TeIU5lY"
      },
      "source": [
        "yy = []\n",
        "for i in y_pred:\n",
        "    yy.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJWcqTG83NvG",
        "outputId": "acec906f-2e05-4352-c32c-9ad872da7816"
      },
      "source": [
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_test,yy,target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.84      1.00      0.91        93\n",
            "     class 1       0.69      0.68      0.68       115\n",
            "     class 2       0.77      0.67      0.71       123\n",
            "\n",
            "    accuracy                           0.76       331\n",
            "   macro avg       0.76      0.78      0.77       331\n",
            "weighted avg       0.76      0.76      0.76       331\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1F75PsE40iF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcNfsMCmG1I-"
      },
      "source": [
        "#NMT example\n",
        "This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.\n",
        "\n",
        "Some sample sentences are available on the sentences variable and are printed on the console.\n",
        "\n",
        "Also, a pre-trained model is available on the model variable and you will use two custom functions to simplify some steps:\n",
        "\n",
        "encode_sequences(): Change texts into sequence of numerical indexes and pad them.\n",
        "translate_many(): Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.\n",
        "For more details on the functions, use help(). The package pandas is loaded as pd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGUZcu0-G2kr"
      },
      "source": [
        "# Transform text into sequence of indexes and pad\n",
        "X = encode_sequences(sentences)\n",
        "\n",
        "# Print the sequences of indexes\n",
        "print(X)\n",
        "\n",
        "# Translate the sentences\n",
        "translated = translate_many(model, X)\n",
        "\n",
        "# Create pandas DataFrame with original and translated\n",
        "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQFQe04MHS4B"
      },
      "source": [
        "#Predict next character\n",
        "In this exercise, you will code the function to predict the next character given a trained model. You will use the past 20 chars to predict the next one. You will learn how to train the model in the next lesson, as this step is integral before model training.\n",
        "\n",
        "This is the initial step to create rules for generating sentences, paragraphs, short texts or other blocks of text as needed.\n",
        "\n",
        "The variables n_vocab, chars_window and the dictionary index_to_char are already loaded in the environment. Also, the functions below are already created for you:\n",
        "\n",
        "initialize_X(): Transforms the text input into a sequence of index numbers with the correct shape.\n",
        "predict_next_char(): Gets the next character using the .predict() method of the model class and the index_to_char dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5kZChjtK7xQ"
      },
      "source": [
        "def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):\n",
        "  \t# Initialize the X vector with zeros\n",
        "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
        "    \n",
        "    # Get next character using the model\n",
        "    next_char = predict_next_char(model, X, index_to_char)\n",
        "\t\n",
        "    return next_char\n",
        "\n",
        "# Define context sentence and print the generated text\n",
        "initial_text = \"I am not insane, \"\n",
        "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6xkQzwYK-UM"
      },
      "source": [
        "#Generate sentence with context\n",
        "In this exercise, you are going to experiment on a pre-trained model for text generation. The model is already loaded in the environment in the model variable, as well as the initialize_params() and get_next_token() functions.\n",
        "\n",
        "This later uses the pre-trained model to predict the next character and return three variables: the next character next_char, the updated sentence res and the the shifted text seq that will be used to predict the next one.\n",
        "\n",
        "You will define a function that receives a pre-trained model and a string that will be the start of the generated sentence as inputs. This is a good practice to generate text with context. The sentence limit of 100 characters is an example, you can use other limits (or even without limit) in your applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wYl5Z_AMEGy"
      },
      "source": [
        "def generate_phrase(model, initial_text):\n",
        "    # Initialize variables  \n",
        "    res, seq, counter, next_char = initialize_params(initial_text)\n",
        "    \n",
        "    # Loop until stop conditions are met\n",
        "    while counter <100 or next_char != r'.':\n",
        "      \t# Get next char using the model and append to the sentence\n",
        "        next_char, res, seq = get_next_token(model, res, seq)\n",
        "        # Update the counter\n",
        "        counter = counter + 1\n",
        "    return res\n",
        "  \n",
        "# Create a phrase\n",
        "print(generate_phrase(model, \"I am not insane, \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzUqQoBJMHQb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6gWKHOXulB5"
      },
      "source": [
        "#Create vectors of sentences and next characters\n",
        "This exercise aims to emphasize more the value of data preparation. You will use texts containing phrases of the character Sheldon from The Big Bang Theory TV show as input and will create vectors of sentence indexes and next characters that are needed before creating a text generation model.\n",
        "\n",
        "The text is available in the sheldon variable, as well as the vocabulary (characters) on the vocabulary variable and the hyperparameters chars_window and step defined with values 20 and 3. This means that a sequence of 20 characters will be used to predict the next one, and the window will shift 3 characters on every iteration.\n",
        "\n",
        "Also, the package pandas as pd is loaded in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4YyrKeul8G"
      },
      "source": [
        "# Instantiate the vectors\n",
        "sentences = []\n",
        "next_chars = []\n",
        "# Loop for every sentence\n",
        "for sentence in sheldon.split('\\n'):\n",
        "    # Get 20 previous chars and next char; then shift by step\n",
        "    for i in range(0, len(sentence) - chars_window, step):\n",
        "        sentences.append(sentence[i:i + chars_window])\n",
        "        next_chars.append(sentence[i + chars_window])\n",
        "\n",
        "# Define a Data Frame with the vectors\n",
        "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
        "\n",
        "# Print the initial rows\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVYd4aoIuoxv"
      },
      "source": [
        "#Preparing the data for training\n",
        "In this exercise, you will continue to prepare the data to train the model. After creating the arrays of sentences and next characters, you need to transform them to numerical values that can be used on the model.\n",
        "\n",
        "This step is necessary because the RNN models expect numbers only and not strings. You will create numerical arrays that have zeros or ones in the positions representing the characters present on the sentences. Ones (or True) represent the corresponding character is present, while zeros (or False) represent the absence of the character in that position of the sentence.\n",
        "\n",
        "The variables sentences, next_char, n_vocab, chars_window, num_seqs (number of sentences in the training data) are already loaded in the environment, as well as numpy as np."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTueUNEOzFey"
      },
      "source": [
        "# Instantiate the variables with zeros\n",
        "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
        "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
        "\n",
        "# Loop for every sentence\n",
        "for i, sentence in enumerate(sentences):\n",
        "  # Loop for every character in sentence\n",
        "  for t, char in enumerate(sentence):\n",
        "    # Set position of the character to 1\n",
        "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
        "    # Set next character to 1\n",
        "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
        "\n",
        "# Print the first position of each\n",
        "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9A92J410AXg"
      },
      "source": [
        "#Creating the text generation model\n",
        "In this exercise, you will define a text generation model using Keras.\n",
        "\n",
        "The variables n_vocab containing the vocabulary size and input_shape containing the shape of the data used for training are already loaded in the environment. Also, the weights of a pre-trained model is available on file model_weights.h5. The model was trained with 40 epochs on the training data. Recap that to train a model in Keras, you just use the method .fit() on the training data (X, y), and the parameter epochs. For example:\n",
        "\n",
        "model.fit(X_train, y_train, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ALFIKJ0A45"
      },
      "source": [
        "# Instantiate the model\n",
        "model = Sequential(name=\"LSTM model\")\n",
        "\n",
        "# Add two LSTM layers\n",
        "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name=\"Input_layer\"))\n",
        "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name=\"LSTM_hidden\"))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(n_vocab, activation='softmax', name=\"Output_layer\"))\n",
        "\n",
        "# Compile and load weights\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.load_weights('model_weights.h5')\n",
        "# Summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP2ip7Ua2LFR"
      },
      "source": [
        "#Preparing the input text\n",
        "You have seen in the video how to prepare the input and output texts. This exercise aims to show a common practice that is to use the maximum length of the sentences to pad all of them, this way no information will be lost.\n",
        "\n",
        "Since the RNN models need the inputs to have the same size, this is a way to pad all sentences and just add zeros to the smaller sentences, without cutting the larger ones.\n",
        "\n",
        "Also, you will use words instead of characters to represent the tokens, this is a common approach for NMT models.\n",
        "\n",
        "The Portuguese texts are loaded on the pt_sentences variable and a fitted tokenizer on the input_tokenizer variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRTGkkjj2Lko"
      },
      "source": [
        "# Get maximum length of the sentences\n",
        "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
        "\n",
        "# Transform text to sequence of numerical indexes\n",
        "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
        "\n",
        "# Pad the sequences\n",
        "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
        "\n",
        "# Print first sentence\n",
        "print(pt_sentences[0])\n",
        "\n",
        "# Print transformed sentence\n",
        "print(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2yXwcdR3ReT"
      },
      "source": [
        "#Preparing the output text\n",
        "In this exercise, you will prepare the output texts to be used on the translation model. Apart from transforming the text to sequences of indexes, you also need to one-hot encode each index.\n",
        "\n",
        "The English texts are loaded on the en_sentences variable, the fitted tokenizer on the output_tokenizer variable and the English vocabulary size on en_vocab_size.\n",
        "\n",
        "Also, a function to perform the first steps of transforming the output language (transformation of texts into sequence of indexes) is already created. The function is loaded on the environment as transform_text_to_sequences() and has two parameters: sentences that expect a list of sentences in English and tokenizer that expects a fitted Tokenizer object from the keras.preprocessing.text module.\n",
        "\n",
        "numpy is loaded as np."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNftLwNm3R9z"
      },
      "source": [
        "# Initialize the variable\n",
        "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
        "\n",
        "# Temporary list\n",
        "ylist = list()\n",
        "for sequence in Y:\n",
        "  \t# One-hot encode sentence and append to list\n",
        "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
        "\n",
        "# Update the variable\n",
        "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
        "\n",
        "# Print the raw sentence and its transformed version\n",
        "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6XoSWdt3T1V"
      },
      "source": [
        "#Translate Portuguese to English\n",
        "This is the last exercise of the course, congratulations on getting here!\n",
        "\n",
        "You will learn how to use NMT models for making translations.\n",
        "\n",
        "A model that encodes Portuguese small phrases and decodes them into English small phrases was pre-trained and is loaded in the model variable.\n",
        "\n",
        "Also, the function predict_one() is already loaded, use help() for details and the dataset is available on the test (raw text) and X_test (tokenized) variables.\n",
        "\n",
        "You will define a function to translate a list of sentences. In the parameters, sentences is a list of phrases to be translated, index_to_word is a dict containing numerical indexes as keys and words as values for the English language, loaded in the en_index_to_word variable.\n",
        "\n",
        "The model summary has been printed for your consideration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-RSLo1t3Y1m"
      },
      "source": [
        "# Function to predict many phrases\n",
        "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Translate the Portuguese sentence\n",
        "        translation = predict_one(model, sentence, index_to_word)\n",
        "        \n",
        "        # Get the raw Portuguese and English sentences\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        \n",
        "        # Print the correct Portuguese and English sentences and the predicted\n",
        "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\n",
        "predict_many(model, X_test[0:10], en_index_to_word, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m927TtYl3ni6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}